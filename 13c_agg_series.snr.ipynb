{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86734369-22ad-49b5-96bd-e24e095bbdfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "version = 'v20250521'\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupKFold,cross_val_predict\n",
    "from skmap.misc import find_files, GoogleSheet, ttprint\n",
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754294b-dcd9-46b1-918b-db90b08d1e68",
   "metadata": {},
   "source": [
    "## creat pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4fa585f-7fa2-459f-a6c0-5ee2483322bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import rasterio\n",
    "# from rasterio.mask import mask\n",
    "# import os\n",
    "# os.environ['USE_PYGEOS'] = '0'\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import Point\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from skmap.misc import find_files, GoogleSheet, ttprint\n",
    "\n",
    "# rtif = 'http://192.168.1.30:8333/ai4sh-landmasked/air.temp/clm_air.temp_era5.copernicus_mx_1km_200..200cm_2019.04.01..2019.04.30_eumap_epsg3035_v0.1.tif'\n",
    "\n",
    "# ttprint('de')\n",
    "# polys = gpd.read_file(\"./material/nuts_de_2021.gpkg\")         \n",
    "# with rasterio.open(rtif) as src:\n",
    "#     # out_image → ndarray (bands, rows, cols)\n",
    "#     # out_transform → affine transform for the masked array\n",
    "#     out_image, out_transform = mask(src, polys.geometry, crop=True)\n",
    "#     out_image = out_image[0]                    # assume one-band raster\n",
    "#     crs = src.crs                               # keep raster CRS\n",
    "\n",
    "# height, width = out_image.shape\n",
    "# rows, cols = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')\n",
    "# xs, ys = rasterio.transform.xy(out_transform, rows, cols, offset='center')\n",
    "# df = pd.DataFrame({\n",
    "#     \"x\": np.array(xs).flatten(),\n",
    "#     \"y\": np.array(ys).flatten(),\n",
    "#     \"value\": out_image.flatten()\n",
    "# })\n",
    "\n",
    "# de = gpd.GeoDataFrame(\n",
    "#         df,\n",
    "#         geometry=[Point(x, y) for x, y in zip(df.x, df.y)],\n",
    "#         crs=crs\n",
    "# )\n",
    "\n",
    "# ttprint('es')\n",
    "# polys = gpd.read_file(\"./material/nuts_es_2021.gpkg\")         \n",
    "# with rasterio.open(rtif) as src:\n",
    "#     out_image, out_transform = mask(src, polys.geometry, crop=True)\n",
    "#     out_image = out_image[0]                    # assume one-band raster\n",
    "#     crs = src.crs                               # keep raster CRS\n",
    "\n",
    "# height, width = out_image.shape\n",
    "# rows, cols = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')\n",
    "# xs, ys = rasterio.transform.xy(out_transform, rows, cols, offset='center')\n",
    "# df = pd.DataFrame({\n",
    "#     \"x\": np.array(xs).flatten(),\n",
    "#     \"y\": np.array(ys).flatten(),\n",
    "#     \"value\": out_image.flatten()\n",
    "# })\n",
    "# es = gpd.GeoDataFrame(\n",
    "#         df,\n",
    "#         geometry=[Point(x, y) for x, y in zip(df.x, df.y)],\n",
    "#         crs=crs\n",
    "# )\n",
    "\n",
    "# ttprint('finish')\n",
    "\n",
    "# de['nuts0'] = 'DE'\n",
    "# es['nuts0'] = 'ES'\n",
    "\n",
    "# agg_pixel = pd.concat([de,es])\n",
    "# agg_pixel = agg_pixel.drop(columns=['geometry','value'])\n",
    "# agg_pixel.to_parquet('./material/pixel_agg.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ddd13-8cf9-4ab9-8edb-799d33b7d206",
   "metadata": {
    "tags": []
   },
   "source": [
    "## read back in the overlaid version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb533c3-89c2-48aa-bb6b-0b8bf787237b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2638770, 393)\n",
      "(1167802, 393)\n"
     ]
    }
   ],
   "source": [
    "tgt = 'soc_log1p'\n",
    "prop = 'soc'\n",
    "# df = pd.read_parquet('./pixel_agg.pq')\n",
    "\n",
    "covs = pd.read_csv(f'./metric/feature_selected_soc_{version}.txt', header=None)[0].tolist() \n",
    "\n",
    "df = pd.read_parquet(f'./material/agg.pixel.tmp_overlaid.2015_{version}.pq')\n",
    "print(df.shape)\n",
    "dff = df.dropna(subset=covs,how='any')\n",
    "print(dff.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5843206-30d7-4a05-a9b2-8692fa36e513",
   "metadata": {},
   "source": [
    "## cast the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c653fd-e9c9-4193-a6c4-1a0e9306a636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:16:48] casting the model\n",
      "[12:17:03] finish casting\n"
     ]
    }
   ],
   "source": [
    "# read in model\n",
    "model = joblib.load(f'./model/model_rf.soc_ccc_{version}.joblib')\n",
    "model.n_jobs = 90\n",
    "\n",
    "# use all the points to train model\n",
    "train = pd.read_parquet(f'./material/soc.topsoil_organized_{version}.pq')\n",
    "train[tgt] = np.log1p(train[prop])\n",
    "train = train.dropna(subset=covs+[tgt],how='any').reset_index(drop=True)\n",
    "\n",
    "# fit\n",
    "model.fit(train[covs], train[tgt])\n",
    "\n",
    "# cast the model\n",
    "import copy\n",
    "from trees_rf import cast_tree_rf, cast_node_rf, pad_leaf_outputs_to_array\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names, but\")\n",
    "ttprint('casting the model')\n",
    "model_copy = copy.deepcopy(model)\n",
    "modeln = cast_node_rf(model_copy, train[covs], train[tgt])\n",
    "ttprint('finish casting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73379f4d-7c0e-4d97-8124-8236f4473404",
   "metadata": {},
   "source": [
    "## start prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa33636-d691-4ae7-aff7-5dc179a2556f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:17:03] start prediction\n",
      "[12:17:04] finish prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_692879/1822879946.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dff[f'pred'] = y_pred\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "ttprint('start prediction')\n",
    "y_pred = model.predict(dff[covs])\n",
    "y_pred = np.expm1(y_pred)\n",
    "dff[f'pred'] = y_pred\n",
    "ttprint('finish prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3e9873-c5f9-4010-b428-302cf63ab179",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:17:05] In total 117 chunks\n",
      "[12:17:05] chunk 1/117 (0:10000)--------\n",
      "[12:17:05] start predicting and padding\n",
      "[12:17:53] start calculating std\n",
      "[12:17:54] chunk 2/117 (10000:20000)--------\n",
      "[12:17:54] start predicting and padding\n",
      "[12:18:42] start calculating std\n",
      "[12:18:42] chunk 3/117 (20000:30000)--------\n",
      "[12:18:42] start predicting and padding\n",
      "[12:19:31] start calculating std\n",
      "[12:19:31] chunk 4/117 (30000:40000)--------\n",
      "[12:19:31] start predicting and padding\n",
      "[12:20:19] start calculating std\n",
      "[12:20:19] chunk 5/117 (40000:50000)--------\n",
      "[12:20:19] start predicting and padding\n",
      "[12:21:08] start calculating std\n",
      "[12:21:08] chunk 6/117 (50000:60000)--------\n",
      "[12:21:08] start predicting and padding\n",
      "[12:21:57] start calculating std\n",
      "[12:21:57] chunk 7/117 (60000:70000)--------\n",
      "[12:21:57] start predicting and padding\n",
      "[12:22:46] start calculating std\n",
      "[12:22:46] chunk 8/117 (70000:80000)--------\n",
      "[12:22:46] start predicting and padding\n",
      "[12:23:35] start calculating std\n",
      "[12:23:35] chunk 9/117 (80000:90000)--------\n",
      "[12:23:35] start predicting and padding\n",
      "[12:24:25] start calculating std\n",
      "[12:24:25] chunk 10/117 (90000:100000)--------\n",
      "[12:24:25] start predicting and padding\n",
      "[12:25:14] start calculating std\n",
      "[12:25:15] chunk 11/117 (100000:110000)--------\n",
      "[12:25:15] start predicting and padding\n",
      "[12:26:04] start calculating std\n",
      "[12:26:04] chunk 12/117 (110000:120000)--------\n",
      "[12:26:04] start predicting and padding\n",
      "[12:26:52] start calculating std\n",
      "[12:26:53] chunk 13/117 (120000:130000)--------\n",
      "[12:26:53] start predicting and padding\n",
      "[12:27:42] start calculating std\n",
      "[12:27:42] chunk 14/117 (130000:140000)--------\n",
      "[12:27:42] start predicting and padding\n",
      "[12:28:31] start calculating std\n",
      "[12:28:31] chunk 15/117 (140000:150000)--------\n",
      "[12:28:31] start predicting and padding\n",
      "[12:29:20] start calculating std\n",
      "[12:29:20] chunk 16/117 (150000:160000)--------\n",
      "[12:29:20] start predicting and padding\n",
      "[12:30:09] start calculating std\n",
      "[12:30:09] chunk 17/117 (160000:170000)--------\n",
      "[12:30:09] start predicting and padding\n",
      "[12:30:58] start calculating std\n",
      "[12:30:59] chunk 18/117 (170000:180000)--------\n",
      "[12:30:59] start predicting and padding\n",
      "[12:31:48] start calculating std\n",
      "[12:31:48] chunk 19/117 (180000:190000)--------\n",
      "[12:31:48] start predicting and padding\n",
      "[12:32:37] start calculating std\n",
      "[12:32:38] chunk 20/117 (190000:200000)--------\n",
      "[12:32:38] start predicting and padding\n",
      "[12:33:27] start calculating std\n",
      "[12:33:27] chunk 21/117 (200000:210000)--------\n",
      "[12:33:27] start predicting and padding\n",
      "[12:34:18] start calculating std\n",
      "[12:34:18] chunk 22/117 (210000:220000)--------\n",
      "[12:34:18] start predicting and padding\n",
      "[12:35:08] start calculating std\n",
      "[12:35:08] chunk 23/117 (220000:230000)--------\n",
      "[12:35:08] start predicting and padding\n",
      "[12:35:58] start calculating std\n",
      "[12:35:58] chunk 24/117 (230000:240000)--------\n",
      "[12:35:58] start predicting and padding\n",
      "[12:36:47] start calculating std\n",
      "[12:36:47] chunk 25/117 (240000:250000)--------\n",
      "[12:36:47] start predicting and padding\n",
      "[12:37:36] start calculating std\n",
      "[12:37:37] chunk 26/117 (250000:260000)--------\n",
      "[12:37:37] start predicting and padding\n",
      "[12:38:26] start calculating std\n",
      "[12:38:27] chunk 27/117 (260000:270000)--------\n",
      "[12:38:27] start predicting and padding\n",
      "[12:39:16] start calculating std\n",
      "[12:39:17] chunk 28/117 (270000:280000)--------\n",
      "[12:39:17] start predicting and padding\n",
      "[12:40:07] start calculating std\n",
      "[12:40:07] chunk 29/117 (280000:290000)--------\n",
      "[12:40:07] start predicting and padding\n",
      "[12:40:57] start calculating std\n",
      "[12:40:58] chunk 30/117 (290000:300000)--------\n",
      "[12:40:58] start predicting and padding\n",
      "[12:41:47] start calculating std\n",
      "[12:41:48] chunk 31/117 (300000:310000)--------\n",
      "[12:41:48] start predicting and padding\n",
      "[12:42:38] start calculating std\n",
      "[12:42:39] chunk 32/117 (310000:320000)--------\n",
      "[12:42:39] start predicting and padding\n",
      "[12:43:29] start calculating std\n",
      "[12:43:30] chunk 33/117 (320000:330000)--------\n",
      "[12:43:30] start predicting and padding\n",
      "[12:44:21] start calculating std\n",
      "[12:44:21] chunk 34/117 (330000:340000)--------\n",
      "[12:44:21] start predicting and padding\n",
      "[12:45:12] start calculating std\n",
      "[12:45:12] chunk 35/117 (340000:350000)--------\n",
      "[12:45:12] start predicting and padding\n",
      "[12:46:02] start calculating std\n",
      "[12:46:03] chunk 36/117 (350000:360000)--------\n",
      "[12:46:03] start predicting and padding\n",
      "[12:46:53] start calculating std\n",
      "[12:46:54] chunk 37/117 (360000:370000)--------\n",
      "[12:46:54] start predicting and padding\n",
      "[12:47:43] start calculating std\n",
      "[12:47:44] chunk 38/117 (370000:380000)--------\n",
      "[12:47:44] start predicting and padding\n",
      "[12:48:34] start calculating std\n",
      "[12:48:34] chunk 39/117 (380000:390000)--------\n",
      "[12:48:34] start predicting and padding\n",
      "[12:49:26] start calculating std\n",
      "[12:49:27] chunk 40/117 (390000:400000)--------\n",
      "[12:49:27] start predicting and padding\n",
      "[12:50:16] start calculating std\n",
      "[12:50:17] chunk 41/117 (400000:410000)--------\n",
      "[12:50:17] start predicting and padding\n",
      "[12:51:07] start calculating std\n",
      "[12:51:08] chunk 42/117 (410000:420000)--------\n",
      "[12:51:08] start predicting and padding\n",
      "[12:51:57] start calculating std\n",
      "[12:51:58] chunk 43/117 (420000:430000)--------\n",
      "[12:51:58] start predicting and padding\n",
      "[12:52:47] start calculating std\n",
      "[12:52:48] chunk 44/117 (430000:440000)--------\n",
      "[12:52:48] start predicting and padding\n",
      "[12:53:36] start calculating std\n",
      "[12:53:37] chunk 45/117 (440000:450000)--------\n",
      "[12:53:37] start predicting and padding\n",
      "[12:54:25] start calculating std\n",
      "[12:54:26] chunk 46/117 (450000:460000)--------\n",
      "[12:54:26] start predicting and padding\n",
      "[12:55:13] start calculating std\n",
      "[12:55:14] chunk 47/117 (460000:470000)--------\n",
      "[12:55:14] start predicting and padding\n",
      "[12:56:02] start calculating std\n",
      "[12:56:03] chunk 48/117 (470000:480000)--------\n",
      "[12:56:03] start predicting and padding\n",
      "[12:56:51] start calculating std\n",
      "[12:56:51] chunk 49/117 (480000:490000)--------\n",
      "[12:56:51] start predicting and padding\n",
      "[12:57:39] start calculating std\n",
      "[12:57:40] chunk 50/117 (490000:500000)--------\n",
      "[12:57:40] start predicting and padding\n",
      "[12:58:29] start calculating std\n",
      "[12:59:19] start calculating std\n",
      "[12:59:19] chunk 52/117 (510000:520000)--------\n",
      "[12:59:19] start predicting and padding\n",
      "[13:00:08] start calculating std\n",
      "[13:00:09] chunk 53/117 (520000:530000)--------\n",
      "[13:00:09] start predicting and padding\n",
      "[13:00:57] start calculating std\n",
      "[13:00:58] chunk 54/117 (530000:540000)--------\n",
      "[13:00:58] start predicting and padding\n",
      "[13:01:47] start calculating std\n",
      "[13:01:47] chunk 55/117 (540000:550000)--------\n",
      "[13:01:47] start predicting and padding\n",
      "[13:02:37] start calculating std\n",
      "[13:02:37] chunk 56/117 (550000:560000)--------\n",
      "[13:02:37] start predicting and padding\n",
      "[13:03:26] start calculating std\n",
      "[13:03:27] chunk 57/117 (560000:570000)--------\n",
      "[13:03:27] start predicting and padding\n",
      "[13:04:16] start calculating std\n",
      "[13:04:17] chunk 58/117 (570000:580000)--------\n",
      "[13:04:17] start predicting and padding\n",
      "[13:05:06] start calculating std\n",
      "[13:05:06] chunk 59/117 (580000:590000)--------\n",
      "[13:05:06] start predicting and padding\n",
      "[13:05:56] start calculating std\n",
      "[13:05:56] chunk 60/117 (590000:600000)--------\n",
      "[13:05:56] start predicting and padding\n",
      "[13:06:45] start calculating std\n",
      "[13:06:46] chunk 61/117 (600000:610000)--------\n",
      "[13:06:46] start predicting and padding\n",
      "[13:07:35] start calculating std\n",
      "[13:07:36] chunk 62/117 (610000:620000)--------\n",
      "[13:07:36] start predicting and padding\n",
      "[13:08:25] start calculating std\n",
      "[13:08:26] chunk 63/117 (620000:630000)--------\n",
      "[13:08:26] start predicting and padding\n",
      "[13:09:15] start calculating std\n",
      "[13:09:15] chunk 64/117 (630000:640000)--------\n",
      "[13:09:15] start predicting and padding\n",
      "[13:10:05] start calculating std\n",
      "[13:10:05] chunk 65/117 (640000:650000)--------\n",
      "[13:10:05] start predicting and padding\n",
      "[13:10:55] start calculating std\n",
      "[13:10:55] chunk 66/117 (650000:660000)--------\n",
      "[13:10:55] start predicting and padding\n",
      "[13:11:44] start calculating std\n",
      "[13:11:45] chunk 67/117 (660000:670000)--------\n",
      "[13:11:45] start predicting and padding\n",
      "[13:12:34] start calculating std\n",
      "[13:12:35] chunk 68/117 (670000:680000)--------\n",
      "[13:12:35] start predicting and padding\n",
      "[13:13:23] start calculating std\n",
      "[13:13:24] chunk 69/117 (680000:690000)--------\n",
      "[13:13:24] start predicting and padding\n",
      "[13:14:12] start calculating std\n",
      "[13:14:13] chunk 70/117 (690000:700000)--------\n",
      "[13:14:13] start predicting and padding\n",
      "[13:15:01] start calculating std\n",
      "[13:15:02] chunk 71/117 (700000:710000)--------\n",
      "[13:15:02] start predicting and padding\n",
      "[13:15:50] start calculating std\n",
      "[13:15:50] chunk 72/117 (710000:720000)--------\n",
      "[13:15:50] start predicting and padding\n",
      "[13:16:39] start calculating std\n",
      "[13:16:39] chunk 73/117 (720000:730000)--------\n",
      "[13:16:39] start predicting and padding\n",
      "[13:17:27] start calculating std\n",
      "[13:17:27] chunk 74/117 (730000:740000)--------\n",
      "[13:17:27] start predicting and padding\n",
      "[13:18:16] start calculating std\n",
      "[13:18:16] chunk 75/117 (740000:750000)--------\n",
      "[13:18:16] start predicting and padding\n",
      "[13:19:05] start calculating std\n",
      "[13:19:05] chunk 76/117 (750000:760000)--------\n",
      "[13:19:05] start predicting and padding\n",
      "[13:19:54] start calculating std\n",
      "[13:19:55] chunk 77/117 (760000:770000)--------\n",
      "[13:19:55] start predicting and padding\n",
      "[13:20:44] start calculating std\n",
      "[13:20:44] chunk 78/117 (770000:780000)--------\n",
      "[13:20:44] start predicting and padding\n",
      "[13:21:33] start calculating std\n",
      "[13:21:34] chunk 79/117 (780000:790000)--------\n",
      "[13:21:34] start predicting and padding\n",
      "[13:22:22] start calculating std\n",
      "[13:22:22] chunk 80/117 (790000:800000)--------\n",
      "[13:22:22] start predicting and padding\n",
      "[13:23:11] start calculating std\n",
      "[13:23:11] chunk 81/117 (800000:810000)--------\n",
      "[13:23:11] start predicting and padding\n",
      "[13:24:00] start calculating std\n",
      "[13:24:00] chunk 82/117 (810000:820000)--------\n",
      "[13:24:00] start predicting and padding\n",
      "[13:24:49] start calculating std\n",
      "[13:24:49] chunk 83/117 (820000:830000)--------\n",
      "[13:24:49] start predicting and padding\n",
      "[13:25:38] start calculating std\n",
      "[13:25:38] chunk 84/117 (830000:840000)--------\n",
      "[13:25:38] start predicting and padding\n",
      "[13:26:26] start calculating std\n",
      "[13:26:27] chunk 85/117 (840000:850000)--------\n",
      "[13:26:27] start predicting and padding\n",
      "[13:27:15] start calculating std\n",
      "[13:27:15] chunk 86/117 (850000:860000)--------\n",
      "[13:27:15] start predicting and padding\n",
      "[13:28:03] start calculating std\n",
      "[13:28:03] chunk 87/117 (860000:870000)--------\n",
      "[13:28:03] start predicting and padding\n",
      "[13:28:51] start calculating std\n",
      "[13:28:52] chunk 88/117 (870000:880000)--------\n",
      "[13:28:52] start predicting and padding\n",
      "[13:29:40] start calculating std\n",
      "[13:29:40] chunk 89/117 (880000:890000)--------\n",
      "[13:29:40] start predicting and padding\n",
      "[13:30:28] start calculating std\n",
      "[13:30:29] chunk 90/117 (890000:900000)--------\n",
      "[13:30:29] start predicting and padding\n",
      "[13:31:18] start calculating std\n",
      "[13:31:18] chunk 91/117 (900000:910000)--------\n",
      "[13:31:18] start predicting and padding\n",
      "[13:32:07] start calculating std\n",
      "[13:32:07] chunk 92/117 (910000:920000)--------\n",
      "[13:32:07] start predicting and padding\n",
      "[13:32:55] start calculating std\n",
      "[13:32:55] chunk 93/117 (920000:930000)--------\n",
      "[13:32:55] start predicting and padding\n",
      "[13:33:44] start calculating std\n",
      "[13:33:44] chunk 94/117 (930000:940000)--------\n",
      "[13:33:44] start predicting and padding\n",
      "[13:34:32] start calculating std\n",
      "[13:34:33] chunk 95/117 (940000:950000)--------\n",
      "[13:34:33] start predicting and padding\n",
      "[13:35:21] start calculating std\n",
      "[13:35:21] chunk 96/117 (950000:960000)--------\n",
      "[13:35:21] start predicting and padding\n",
      "[13:36:10] start calculating std\n",
      "[13:36:10] chunk 97/117 (960000:970000)--------\n",
      "[13:36:10] start predicting and padding\n",
      "[13:36:59] start calculating std\n",
      "[13:36:59] chunk 98/117 (970000:980000)--------\n",
      "[13:36:59] start predicting and padding\n",
      "[13:37:48] start calculating std\n",
      "[13:37:48] chunk 99/117 (980000:990000)--------\n",
      "[13:37:48] start predicting and padding\n",
      "[13:38:37] start calculating std\n",
      "[13:38:37] chunk 100/117 (990000:1000000)--------\n",
      "[13:38:37] start predicting and padding\n",
      "[13:39:26] start calculating std\n",
      "[13:39:26] chunk 101/117 (1000000:1010000)--------\n",
      "[13:39:26] start predicting and padding\n",
      "[13:40:15] start calculating std\n",
      "[13:40:15] chunk 102/117 (1010000:1020000)--------\n",
      "[13:40:15] start predicting and padding\n",
      "[13:41:04] start calculating std\n",
      "[13:41:04] chunk 103/117 (1020000:1030000)--------\n",
      "[13:41:04] start predicting and padding\n",
      "[13:41:53] start calculating std\n",
      "[13:41:53] chunk 104/117 (1030000:1040000)--------\n",
      "[13:41:53] start predicting and padding\n",
      "[13:42:42] start calculating std\n",
      "[13:42:42] chunk 105/117 (1040000:1050000)--------\n",
      "[13:42:42] start predicting and padding\n",
      "[13:43:30] start calculating std\n",
      "[13:43:31] chunk 106/117 (1050000:1060000)--------\n",
      "[13:43:31] start predicting and padding\n",
      "[13:44:19] start calculating std\n",
      "[13:44:19] chunk 107/117 (1060000:1070000)--------\n",
      "[13:44:19] start predicting and padding\n",
      "[13:45:08] start calculating std\n",
      "[13:45:08] chunk 108/117 (1070000:1080000)--------\n",
      "[13:45:08] start predicting and padding\n",
      "[13:45:56] start calculating std\n",
      "[13:45:56] chunk 109/117 (1080000:1090000)--------\n",
      "[13:45:56] start predicting and padding\n",
      "[13:46:44] start calculating std\n",
      "[13:46:45] chunk 110/117 (1090000:1100000)--------\n",
      "[13:46:45] start predicting and padding\n",
      "[13:47:32] start calculating std\n",
      "[13:47:33] chunk 111/117 (1100000:1110000)--------\n",
      "[13:47:33] start predicting and padding\n",
      "[13:48:20] start calculating std\n",
      "[13:48:21] chunk 112/117 (1110000:1120000)--------\n",
      "[13:48:21] start predicting and padding\n",
      "[13:49:09] start calculating std\n",
      "[13:49:09] chunk 113/117 (1120000:1130000)--------\n",
      "[13:49:09] start predicting and padding\n",
      "[13:49:58] start calculating std\n",
      "[13:49:58] chunk 114/117 (1130000:1140000)--------\n",
      "[13:49:58] start predicting and padding\n",
      "[13:50:46] start calculating std\n",
      "[13:50:46] chunk 115/117 (1140000:1150000)--------\n",
      "[13:50:46] start predicting and padding\n",
      "[13:51:35] start calculating std\n",
      "[13:51:35] chunk 116/117 (1150000:1160000)--------\n",
      "[13:51:35] start predicting and padding\n",
      "[13:52:24] start calculating std\n",
      "[13:52:24] chunk 117/117 (1160000:1167802)--------\n",
      "[13:52:24] start predicting and padding\n",
      "[13:53:01] start calculating std\n",
      "[13:53:01] finish all-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_692879/2775512733.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dff['pred_std'] = np.concatenate(pred_std_chunks)\n"
     ]
    }
   ],
   "source": [
    "# do the predictions in chunck\n",
    "chunk_size = 10000\n",
    "num_chunks = int(np.ceil(len(dff) / chunk_size))\n",
    "\n",
    "pred_std_chunks = []\n",
    "\n",
    "ttprint(f'In total {num_chunks} chunks')\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = min((i + 1) * chunk_size, len(dff))\n",
    "    ttprint(f'chunk {i + 1}/{num_chunks} ({start}:{end})--------')\n",
    "\n",
    "    chunk = dff.iloc[start:end]\n",
    "\n",
    "    ttprint('start predicting and padding')\n",
    "    node_preds_chunk = modeln.predict(chunk[covs])\n",
    "    nodes_chunk = pad_leaf_outputs_to_array(node_preds_chunk, pad_value=np.nan)\n",
    "    nodes_chunk = np.expm1(nodes_chunk)\n",
    "    \n",
    "    ttprint('start calculating std')\n",
    "    std_chunk = np.nanstd(nodes_chunk.T, axis=0)\n",
    "    pred_std_chunks.append(std_chunk)\n",
    "\n",
    "ttprint(f'finish all-------------------------------')\n",
    "\n",
    "dff['pred_std'] = np.concatenate(pred_std_chunks)\n",
    "\n",
    "dff.to_parquet(f'./material/pixel_agg.predicted_2015.{version}.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfda959-af14-4846-b040-3cc7145e280f",
   "metadata": {},
   "source": [
    "## join with the other years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d02da91-c87b-47c4-9cd6-d823aecbdc6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dfo = pd.read_parquet(f'./material/pixel_agg.predicted_2015.{version}.pq')\n",
    "dfo = dfo[['lon','lat','pred','pred_std']]\n",
    "dfo =dfo.rename(columns={\"pred\":\"pred_2015\",\"pred_std\":\"pred_std_2015\"})\n",
    "geometry = [Point(xy) for xy in zip(dfo['lon'], dfo['lat'])]\n",
    "dfo = gpd.GeoDataFrame(dfo, geometry=geometry, crs=\"EPSG:3035\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e71ca-14a0-4204-82b0-c66dd011eea8",
   "metadata": {},
   "source": [
    "## aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b2d973f-77e3-4cc8-a500-0c247d1b0244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# point\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "# correlation function\n",
    "def exponential_model(h, nugget, sill, range_):\n",
    "    return nugget + (sill - nugget) * (1 - np.exp(-h / range_))\n",
    "\n",
    "def correlation_function(h, nugget, sill, range_):\n",
    "    # Calculate the variogram value at distance h\n",
    "    gamma_h = exponential_model(h, nugget, sill, range_)\n",
    "    # Calculate and return the correlation function value\n",
    "    return (sill - gamma_h) / sill\n",
    "\n",
    "params_es = np.array([0.74285565, 1.00000001, 5.99345285])\n",
    "params_de = np.array([0.72415892,  1.00000001, 10.60109839])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "729b7300-a56b-4cf9-a22e-a8d0ffbc6c52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:05:20] 2.5 -------------------------------\n",
      "[20:36:12] 10 -------------------------------\n",
      "[21:03:13] 40 -------------------------------\n",
      "[21:06:20] 200 -------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import HuberRegressor, TheilSenRegressor\n",
    "\n",
    "tsr = TheilSenRegressor(random_state=42)\n",
    "df_base = dfo.copy()\n",
    "\n",
    "sizes = [2.5, 10, 40, 200]\n",
    "\n",
    "for isize in sizes:\n",
    "    ttprint(isize, '-------------------------------')\n",
    "    plg_id_col = f'id_{isize}'\n",
    "    \n",
    "    plg = gpd.read_file(f'./material/grid_all.{isize}km_agg.vlz.{version}.gpkg')\n",
    "    plg = plg[['CNTR_CODE', 'var_2009', 'var_2018', 'mean_2009', 'mean_2018', \n",
    "               'change', 'signal', 'noise','snr', 'geometry']]\n",
    "    df = gpd.sjoin(df_base, plg, how=\"inner\", predicate=\"intersects\")\n",
    "    df = df.rename(columns={'index_right':f'id_{isize}','CNTR_CODE':'nuts0'})\n",
    "    plg = plg.rename(columns={'signal':'signal_change','noise':'noise_change','snr':'snr_change'})\n",
    "    \n",
    "    group_ids = df[plg_id_col].dropna().unique().tolist()\n",
    "\n",
    "    for id_val in group_ids:\n",
    "        ipnt = df.loc[df[plg_id_col] == id_val]\n",
    "        if ipnt.empty:\n",
    "            continue\n",
    "        \n",
    "        params_vg = params_es if ipnt.iloc[0]['nuts0'] == 'ES' else params_de\n",
    " \n",
    "        # distance\n",
    "        coords = np.array([(geom.x, geom.y) for geom in ipnt.geometry])\n",
    "        dist_matrix = distance_matrix(coords, coords) / 1000\n",
    "        corr_matrix = correlation_function(dist_matrix, *params_vg)\n",
    "        np.fill_diagonal(corr_matrix, 1.0)\n",
    "        # variances\n",
    "        std15 = ipnt['pred_std_2015'].to_numpy()\n",
    "        var_matrix_2015 = np.outer(std15, std15) * corr_matrix\n",
    "        n = len(ipnt)\n",
    "        # aggregation\n",
    "        agg_var_2015 = np.nansum(var_matrix_2015) / (n * n)\n",
    "        m15 = ipnt['pred_2015'].mean()\n",
    "        plg.loc[plg.index == id_val, 'var_2015'] = agg_var_2015\n",
    "        plg.loc[plg.index == id_val, 'mean_2015'] = m15\n",
    "        \n",
    "        # linear trend fitting\n",
    "        soc_p = plg.loc[plg.index == id_val, ['mean_2009','mean_2015','mean_2018']].values.squeeze()\n",
    "        soc_std = np.sqrt(plg.loc[plg.index == id_val, ['var_2009','var_2015','var_2018']].values.squeeze())\n",
    "\n",
    "        # fit linear slope\n",
    "        X = np.array([0,6,9]) # 2009, 2015, 2018\n",
    "        # # for observations\n",
    "        # tsr.fit(X.reshape(-1, 1), np.array(soc_s))\n",
    "        # slope_sample.append(tsr.coef_[0])\n",
    "        # intercept_sample.append(tsr.intercept_)\n",
    "        # score_sample.append(tsr.score(X.reshape(-1, 1), np.array(soc_s)))\n",
    "        # # for prediction\n",
    "        y = np.array(soc_p, dtype=float)\n",
    "        mask = np.isfinite(y)  # keep only finite (non-NaN, non-inf) values\n",
    "        if mask.sum() > 2:    # need at least 2 points to fit a line\n",
    "            tsr.fit(X[mask].reshape(-1, 1), y[mask])\n",
    "            tsr.fit(X.reshape(-1, 1), np.array(soc_p))\n",
    "            # intercept_pred.append(tsr.intercept_)\n",
    "            # score_pred.append(tsr.score(X.reshape(-1, 1), np.array(soc_p)))\n",
    "\n",
    "            # shape becomes (n_realizations, n_time_steps), e.g., (100, 3)\n",
    "            n_realizations = 100\n",
    "            rt = np.random.normal(loc=soc_p, scale=soc_std, size=(n_realizations, 3))\n",
    "            slope_r = []\n",
    "\n",
    "            for i in range(100):  # number of Monte Carlo samples\n",
    "                # Sample one realization (index) per time step\n",
    "                # Choose a random index from 0 to 119 (assuming 120 realizations)\n",
    "                sample_indices = np.random.randint(0, rt.shape[0], size=rt.shape[1])\n",
    "                ss = np.array([rt[sample_indices[j], j] for j in range(rt.shape[1])])  # shape (n_time_steps,)\n",
    "\n",
    "                tsr.fit(X.reshape(-1, 1), ss)\n",
    "                slope_r.append(tsr.coef_[0])\n",
    "                # score_r.append(tsr.score(X.reshape(-1, 1), ss))\n",
    "\n",
    "            plg.loc[plg.index == id_val, 'noise_series'] = np.std(slope_r)\n",
    "            plg.loc[plg.index == id_val, 'trend'] = tsr.coef_[0]\n",
    "\n",
    "        else:\n",
    "            plg.loc[plg.index == id_val, 'noise_series'] = np.nan\n",
    "            plg.loc[plg.index == id_val, 'trend'] = np.nan\n",
    "            \n",
    "    # Merge back into plg\n",
    "    plg['signal_series'] = plg['trend'].abs()\n",
    "    plg['snr_series']  = plg['signal_series']/plg['noise_series']\n",
    "\n",
    "    plg.to_file(f'./material/grid_all.{isize}km_agg.srs.{version}.gpkg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "273e2aa4-59cb-47c2-8c76-d6fe673f6f80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CNTR_CODE', 'var_2009', 'var_2018', 'mean_2009', 'mean_2018', 'change',\n",
       "       'signal_change', 'noise_change', 'snr_change', 'geometry', 'var_2015',\n",
       "       'mean_2015', 'noise_series', 'trend', 'signal_series', 'snr_series'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e7a63-8921-4eb8-92f5-ab2321ffc192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
