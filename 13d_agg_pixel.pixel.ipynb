{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86734369-22ad-49b5-96bd-e24e095bbdfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "version = 'v20250521'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754294b-dcd9-46b1-918b-db90b08d1e68",
   "metadata": {},
   "source": [
    "## creat pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4fa585f-7fa2-459f-a6c0-5ee2483322bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import rasterio\n",
    "# from rasterio.mask import mask\n",
    "# import os\n",
    "# os.environ['USE_PYGEOS'] = '0'\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import Point\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from skmap.misc import find_files, GoogleSheet, ttprint\n",
    "\n",
    "# rtif = 'http://192.168.1.30:8333/ai4sh-landmasked/air.temp/clm_air.temp_era5.copernicus_mx_1km_200..200cm_2019.04.01..2019.04.30_eumap_epsg3035_v0.1.tif'\n",
    "\n",
    "# ttprint('de')\n",
    "# polys = gpd.read_file(\"./material/nuts_de_2021.gpkg\")         \n",
    "# with rasterio.open(rtif) as src:\n",
    "#     # out_image → ndarray (bands, rows, cols)\n",
    "#     # out_transform → affine transform for the masked array\n",
    "#     out_image, out_transform = mask(src, polys.geometry, crop=True)\n",
    "#     out_image = out_image[0]                    # assume one-band raster\n",
    "#     crs = src.crs                               # keep raster CRS\n",
    "\n",
    "# height, width = out_image.shape\n",
    "# rows, cols = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')\n",
    "# xs, ys = rasterio.transform.xy(out_transform, rows, cols, offset='center')\n",
    "# df = pd.DataFrame({\n",
    "#     \"x\": np.array(xs).flatten(),\n",
    "#     \"y\": np.array(ys).flatten(),\n",
    "#     \"value\": out_image.flatten()\n",
    "# })\n",
    "\n",
    "# de = gpd.GeoDataFrame(\n",
    "#         df,\n",
    "#         geometry=[Point(x, y) for x, y in zip(df.x, df.y)],\n",
    "#         crs=crs\n",
    "# )\n",
    "\n",
    "# ttprint('es')\n",
    "# polys = gpd.read_file(\"./material/nuts_es_2021.gpkg\")         \n",
    "# with rasterio.open(rtif) as src:\n",
    "#     out_image, out_transform = mask(src, polys.geometry, crop=True)\n",
    "#     out_image = out_image[0]                    # assume one-band raster\n",
    "#     crs = src.crs                               # keep raster CRS\n",
    "\n",
    "# height, width = out_image.shape\n",
    "# rows, cols = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')\n",
    "# xs, ys = rasterio.transform.xy(out_transform, rows, cols, offset='center')\n",
    "# df = pd.DataFrame({\n",
    "#     \"x\": np.array(xs).flatten(),\n",
    "#     \"y\": np.array(ys).flatten(),\n",
    "#     \"value\": out_image.flatten()\n",
    "# })\n",
    "# es = gpd.GeoDataFrame(\n",
    "#         df,\n",
    "#         geometry=[Point(x, y) for x, y in zip(df.x, df.y)],\n",
    "#         crs=crs\n",
    "# )\n",
    "\n",
    "# ttprint('finish')\n",
    "\n",
    "# de['nuts0'] = 'DE'\n",
    "# es['nuts0'] = 'ES'\n",
    "\n",
    "# agg_pixel = pd.concat([de,es])\n",
    "# agg_pixel = agg_pixel.drop(columns=['geometry','value'])\n",
    "# agg_pixel.to_parquet('./material/pixel_agg.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ddd13-8cf9-4ab9-8edb-799d33b7d206",
   "metadata": {
    "tags": []
   },
   "source": [
    "## read back in the overlaid version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b39ca5-c764-4903-bbf8-cd5586fd59bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7678990, 79)\n",
      "(2335589, 79)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupKFold,cross_val_predict\n",
    "from skmap.misc import find_files, GoogleSheet, ttprint\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "prop = 'soc'\n",
    "tgt = 'soc_log1p'\n",
    "\n",
    "# df = pd.read_parquet('./pixel_agg.pq')\n",
    "\n",
    "covs = pd.read_csv(f'./feature_selected_soc_{version}.txt', header=None)[0].tolist() \n",
    "\n",
    "df = pd.read_parquet(f'./agg.pixel.tmp_overlaid_{version}.pq')\n",
    "print(df.shape)\n",
    "dff = df.dropna(subset=covs,how='any')\n",
    "print(dff.shape)\n",
    "# dff = dff.groupby('id').filter(lambda x: len(x) >= 3).reset_index(drop=True)\n",
    "# print(dff.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5843206-30d7-4a05-a9b2-8692fa36e513",
   "metadata": {},
   "source": [
    "## cast the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c653fd-e9c9-4193-a6c4-1a0e9306a636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:18:17] casting the model\n",
      "[14:18:40] finish casting\n"
     ]
    }
   ],
   "source": [
    "# read in model\n",
    "model = joblib.load(f'./model_rf.soc_ccc_{version}.joblib')\n",
    "model.n_jobs = 90\n",
    "\n",
    "# use all the points to train model\n",
    "train = pd.read_parquet(f'./soc.topsoil_organized_{version}.pq')\n",
    "train[tgt] = np.log1p(train[prop])\n",
    "train = train.dropna(subset=covs+[tgt],how='any').reset_index(drop=True)\n",
    "\n",
    "# fit\n",
    "model.fit(train[covs], train[tgt])\n",
    "\n",
    "# cast the model\n",
    "import copy\n",
    "from trees_rf import cast_tree_rf, cast_node_rf, pad_leaf_outputs_to_array\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names, but\")\n",
    "ttprint('casting the model')\n",
    "model_copy = copy.deepcopy(model)\n",
    "modeln = cast_node_rf(model_copy, train[covs], train[tgt])\n",
    "ttprint('finish casting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73379f4d-7c0e-4d97-8124-8236f4473404",
   "metadata": {},
   "source": [
    "## start prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efa33636-d691-4ae7-aff7-5dc179a2556f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:18:40] start prediction\n",
      "[14:18:41] finish prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65218/1822879946.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dff[f'pred'] = y_pred\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "ttprint('start prediction')\n",
    "y_pred = model.predict(dff[covs])\n",
    "y_pred = np.expm1(y_pred)\n",
    "dff[f'pred'] = y_pred\n",
    "ttprint('finish prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d3e9873-c5f9-4010-b428-302cf63ab179",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:18:41] In total 234 chunks\n",
      "[14:18:41] chunk 1/234 (0:10000)--------\n",
      "[14:18:41] start predicting and padding\n",
      "[14:19:34] start calculating std\n",
      "[14:19:34] chunk 2/234 (10000:20000)--------\n",
      "[14:19:34] start predicting and padding\n",
      "[14:20:27] start calculating std\n",
      "[14:20:27] chunk 3/234 (20000:30000)--------\n",
      "[14:20:27] start predicting and padding\n",
      "[14:21:19] start calculating std\n",
      "[14:21:20] chunk 4/234 (30000:40000)--------\n",
      "[14:21:20] start predicting and padding\n",
      "[14:22:12] start calculating std\n",
      "[14:22:13] chunk 5/234 (40000:50000)--------\n",
      "[14:22:13] start predicting and padding\n",
      "[14:23:05] start calculating std\n",
      "[14:23:05] chunk 6/234 (50000:60000)--------\n",
      "[14:23:05] start predicting and padding\n",
      "[14:23:58] start calculating std\n",
      "[14:23:58] chunk 7/234 (60000:70000)--------\n",
      "[14:23:58] start predicting and padding\n",
      "[14:24:51] start calculating std\n",
      "[14:24:51] chunk 8/234 (70000:80000)--------\n",
      "[14:24:51] start predicting and padding\n",
      "[14:25:44] start calculating std\n",
      "[14:25:45] chunk 9/234 (80000:90000)--------\n",
      "[14:25:45] start predicting and padding\n",
      "[14:26:37] start calculating std\n",
      "[14:26:38] chunk 10/234 (90000:100000)--------\n",
      "[14:26:38] start predicting and padding\n",
      "[14:27:31] start calculating std\n",
      "[14:27:31] chunk 11/234 (100000:110000)--------\n",
      "[14:27:31] start predicting and padding\n",
      "[14:28:24] start calculating std\n",
      "[14:28:24] chunk 12/234 (110000:120000)--------\n",
      "[14:28:24] start predicting and padding\n",
      "[14:29:17] start calculating std\n",
      "[14:29:18] chunk 13/234 (120000:130000)--------\n",
      "[14:29:18] start predicting and padding\n",
      "[14:30:11] start calculating std\n",
      "[14:30:11] chunk 14/234 (130000:140000)--------\n",
      "[14:30:11] start predicting and padding\n",
      "[14:31:04] start calculating std\n",
      "[14:31:04] chunk 15/234 (140000:150000)--------\n",
      "[14:31:04] start predicting and padding\n",
      "[14:31:57] start calculating std\n",
      "[14:31:58] chunk 16/234 (150000:160000)--------\n",
      "[14:31:58] start predicting and padding\n",
      "[14:32:51] start calculating std\n",
      "[14:32:51] chunk 17/234 (160000:170000)--------\n",
      "[14:32:51] start predicting and padding\n",
      "[14:33:45] start calculating std\n",
      "[14:33:45] chunk 18/234 (170000:180000)--------\n",
      "[14:33:45] start predicting and padding\n",
      "[14:34:38] start calculating std\n",
      "[14:34:39] chunk 19/234 (180000:190000)--------\n",
      "[14:34:39] start predicting and padding\n",
      "[14:35:32] start calculating std\n",
      "[14:35:32] chunk 20/234 (190000:200000)--------\n",
      "[14:35:32] start predicting and padding\n",
      "[14:36:26] start calculating std\n",
      "[14:36:27] chunk 21/234 (200000:210000)--------\n",
      "[14:36:27] start predicting and padding\n",
      "[14:37:21] start calculating std\n",
      "[14:37:21] chunk 22/234 (210000:220000)--------\n",
      "[14:37:21] start predicting and padding\n",
      "[14:38:15] start calculating std\n",
      "[14:38:16] chunk 23/234 (220000:230000)--------\n",
      "[14:38:16] start predicting and padding\n",
      "[14:39:10] start calculating std\n",
      "[14:39:10] chunk 24/234 (230000:240000)--------\n",
      "[14:39:10] start predicting and padding\n",
      "[14:40:04] start calculating std\n",
      "[14:40:04] chunk 25/234 (240000:250000)--------\n",
      "[14:40:04] start predicting and padding\n",
      "[14:40:58] start calculating std\n",
      "[14:40:58] chunk 26/234 (250000:260000)--------\n",
      "[14:40:58] start predicting and padding\n",
      "[14:41:52] start calculating std\n",
      "[14:41:53] chunk 27/234 (260000:270000)--------\n",
      "[14:41:53] start predicting and padding\n",
      "[14:42:47] start calculating std\n",
      "[14:42:48] chunk 28/234 (270000:280000)--------\n",
      "[14:42:48] start predicting and padding\n",
      "[14:43:43] start calculating std\n",
      "[14:43:44] chunk 29/234 (280000:290000)--------\n",
      "[14:43:44] start predicting and padding\n",
      "[14:44:38] start calculating std\n",
      "[14:44:39] chunk 30/234 (290000:300000)--------\n",
      "[14:44:39] start predicting and padding\n",
      "[14:45:34] start calculating std\n",
      "[14:45:35] chunk 31/234 (300000:310000)--------\n",
      "[14:45:35] start predicting and padding\n",
      "[14:46:30] start calculating std\n",
      "[14:46:31] chunk 32/234 (310000:320000)--------\n",
      "[14:46:31] start predicting and padding\n",
      "[14:47:26] start calculating std\n",
      "[14:47:27] chunk 33/234 (320000:330000)--------\n",
      "[14:47:27] start predicting and padding\n",
      "[14:48:22] start calculating std\n",
      "[14:48:23] chunk 34/234 (330000:340000)--------\n",
      "[14:48:23] start predicting and padding\n",
      "[14:49:18] start calculating std\n",
      "[14:49:19] chunk 35/234 (340000:350000)--------\n",
      "[14:49:19] start predicting and padding\n",
      "[14:50:13] start calculating std\n",
      "[14:50:14] chunk 36/234 (350000:360000)--------\n",
      "[14:50:14] start predicting and padding\n",
      "[14:51:09] start calculating std\n",
      "[14:51:10] chunk 37/234 (360000:370000)--------\n",
      "[14:51:10] start predicting and padding\n",
      "[14:52:05] start calculating std\n",
      "[14:52:06] chunk 38/234 (370000:380000)--------\n",
      "[14:52:06] start predicting and padding\n",
      "[14:53:01] start calculating std\n",
      "[14:53:02] chunk 39/234 (380000:390000)--------\n",
      "[14:53:02] start predicting and padding\n",
      "[14:53:57] start calculating std\n",
      "[14:53:57] chunk 40/234 (390000:400000)--------\n",
      "[14:53:57] start predicting and padding\n",
      "[14:54:52] start calculating std\n",
      "[14:54:53] chunk 41/234 (400000:410000)--------\n",
      "[14:54:53] start predicting and padding\n",
      "[14:55:47] start calculating std\n",
      "[14:55:47] chunk 42/234 (410000:420000)--------\n",
      "[14:55:47] start predicting and padding\n",
      "[14:56:42] start calculating std\n",
      "[14:56:43] chunk 43/234 (420000:430000)--------\n",
      "[14:56:43] start predicting and padding\n",
      "[14:57:36] start calculating std\n",
      "[14:57:37] chunk 44/234 (430000:440000)--------\n",
      "[14:57:37] start predicting and padding\n",
      "[14:58:30] start calculating std\n",
      "[14:58:30] chunk 45/234 (440000:450000)--------\n",
      "[14:58:30] start predicting and padding\n",
      "[14:59:23] start calculating std\n",
      "[14:59:24] chunk 46/234 (450000:460000)--------\n",
      "[14:59:24] start predicting and padding\n",
      "[15:00:16] start calculating std\n",
      "[15:00:17] chunk 47/234 (460000:470000)--------\n",
      "[15:00:17] start predicting and padding\n",
      "[15:01:09] start calculating std\n",
      "[15:01:10] chunk 48/234 (470000:480000)--------\n",
      "[15:01:10] start predicting and padding\n",
      "[15:02:02] start calculating std\n",
      "[15:02:02] chunk 49/234 (480000:490000)--------\n",
      "[15:02:02] start predicting and padding\n",
      "[15:02:55] start calculating std\n",
      "[15:02:56] chunk 50/234 (490000:500000)--------\n",
      "[15:02:56] start predicting and padding\n",
      "[15:03:49] start calculating std\n",
      "[15:03:49] chunk 51/234 (500000:510000)--------\n",
      "[15:03:49] start predicting and padding\n",
      "[15:04:43] start calculating std\n",
      "[15:04:44] chunk 52/234 (510000:520000)--------\n",
      "[15:04:44] start predicting and padding\n",
      "[15:05:37] start calculating std\n",
      "[15:05:37] chunk 53/234 (520000:530000)--------\n",
      "[15:05:37] start predicting and padding\n",
      "[15:06:30] start calculating std\n",
      "[15:06:31] chunk 54/234 (530000:540000)--------\n",
      "[15:06:31] start predicting and padding\n",
      "[15:07:24] start calculating std\n",
      "[15:07:25] chunk 55/234 (540000:550000)--------\n",
      "[15:07:25] start predicting and padding\n",
      "[15:08:19] start calculating std\n",
      "[15:08:20] chunk 56/234 (550000:560000)--------\n",
      "[15:08:20] start predicting and padding\n",
      "[15:09:14] start calculating std\n",
      "[15:09:15] chunk 57/234 (560000:570000)--------\n",
      "[15:09:15] start predicting and padding\n",
      "[15:10:09] start calculating std\n",
      "[15:10:10] chunk 58/234 (570000:580000)--------\n",
      "[15:10:10] start predicting and padding\n",
      "[15:11:03] start calculating std\n",
      "[15:11:04] chunk 59/234 (580000:590000)--------\n",
      "[15:11:04] start predicting and padding\n",
      "[15:11:57] start calculating std\n",
      "[15:11:58] chunk 60/234 (590000:600000)--------\n",
      "[15:11:58] start predicting and padding\n",
      "[15:12:52] start calculating std\n",
      "[15:12:53] chunk 61/234 (600000:610000)--------\n",
      "[15:12:53] start predicting and padding\n",
      "[15:13:47] start calculating std\n",
      "[15:13:48] chunk 62/234 (610000:620000)--------\n",
      "[15:13:48] start predicting and padding\n",
      "[15:14:41] start calculating std\n",
      "[15:14:42] chunk 63/234 (620000:630000)--------\n",
      "[15:14:42] start predicting and padding\n",
      "[15:15:36] start calculating std\n",
      "[15:15:37] chunk 64/234 (630000:640000)--------\n",
      "[15:15:37] start predicting and padding\n",
      "[15:16:31] start calculating std\n",
      "[15:16:32] chunk 65/234 (640000:650000)--------\n",
      "[15:16:32] start predicting and padding\n",
      "[15:17:26] start calculating std\n",
      "[15:17:27] chunk 66/234 (650000:660000)--------\n",
      "[15:17:27] start predicting and padding\n",
      "[15:18:20] start calculating std\n",
      "[15:18:21] chunk 67/234 (660000:670000)--------\n",
      "[15:18:21] start predicting and padding\n",
      "[15:19:14] start calculating std\n",
      "[15:19:15] chunk 68/234 (670000:680000)--------\n",
      "[15:19:15] start predicting and padding\n",
      "[15:20:09] start calculating std\n",
      "[15:20:09] chunk 69/234 (680000:690000)--------\n",
      "[15:20:09] start predicting and padding\n",
      "[15:21:03] start calculating std\n",
      "[15:21:03] chunk 70/234 (690000:700000)--------\n",
      "[15:21:03] start predicting and padding\n",
      "[15:21:56] start calculating std\n",
      "[15:21:57] chunk 71/234 (700000:710000)--------\n",
      "[15:21:57] start predicting and padding\n",
      "[15:22:50] start calculating std\n",
      "[15:22:50] chunk 72/234 (710000:720000)--------\n",
      "[15:22:50] start predicting and padding\n",
      "[15:23:43] start calculating std\n",
      "[15:23:44] chunk 73/234 (720000:730000)--------\n",
      "[15:23:44] start predicting and padding\n",
      "[15:24:37] start calculating std\n",
      "[15:24:37] chunk 74/234 (730000:740000)--------\n",
      "[15:24:37] start predicting and padding\n",
      "[15:25:30] start calculating std\n",
      "[15:25:31] chunk 75/234 (740000:750000)--------\n",
      "[15:25:31] start predicting and padding\n",
      "[15:26:24] start calculating std\n",
      "[15:26:24] chunk 76/234 (750000:760000)--------\n",
      "[15:26:24] start predicting and padding\n",
      "[15:27:18] start calculating std\n",
      "[15:27:18] chunk 77/234 (760000:770000)--------\n",
      "[15:27:18] start predicting and padding\n",
      "[15:28:11] start calculating std\n",
      "[15:28:11] chunk 78/234 (770000:780000)--------\n",
      "[15:28:11] start predicting and padding\n",
      "[15:29:05] start calculating std\n",
      "[15:29:05] chunk 79/234 (780000:790000)--------\n",
      "[15:29:05] start predicting and padding\n",
      "[15:29:58] start calculating std\n",
      "[15:29:58] chunk 80/234 (790000:800000)--------\n",
      "[15:29:58] start predicting and padding\n",
      "[15:30:52] start calculating std\n",
      "[15:30:52] chunk 81/234 (800000:810000)--------\n",
      "[15:30:52] start predicting and padding\n",
      "[15:31:45] start calculating std\n",
      "[15:31:45] chunk 82/234 (810000:820000)--------\n",
      "[15:31:45] start predicting and padding\n",
      "[15:32:39] start calculating std\n",
      "[15:32:39] chunk 83/234 (820000:830000)--------\n",
      "[15:32:39] start predicting and padding\n",
      "[15:33:33] start calculating std\n",
      "[15:33:33] chunk 84/234 (830000:840000)--------\n",
      "[15:33:33] start predicting and padding\n",
      "[15:34:28] start calculating std\n",
      "[15:34:29] chunk 85/234 (840000:850000)--------\n",
      "[15:34:29] start predicting and padding\n",
      "[15:35:24] start calculating std\n",
      "[15:35:24] chunk 86/234 (850000:860000)--------\n",
      "[15:35:24] start predicting and padding\n",
      "[15:36:19] start calculating std\n",
      "[15:36:19] chunk 87/234 (860000:870000)--------\n",
      "[15:36:19] start predicting and padding\n",
      "[15:37:14] start calculating std\n",
      "[15:37:14] chunk 88/234 (870000:880000)--------\n",
      "[15:37:14] start predicting and padding\n",
      "[15:38:09] start calculating std\n",
      "[15:38:10] chunk 89/234 (880000:890000)--------\n",
      "[15:38:10] start predicting and padding\n",
      "[15:39:05] start calculating std\n",
      "[15:39:05] chunk 90/234 (890000:900000)--------\n",
      "[15:39:05] start predicting and padding\n",
      "[15:40:00] start calculating std\n",
      "[15:40:01] chunk 91/234 (900000:910000)--------\n",
      "[15:40:01] start predicting and padding\n",
      "[15:40:56] start calculating std\n",
      "[15:40:56] chunk 92/234 (910000:920000)--------\n",
      "[15:40:56] start predicting and padding\n",
      "[15:41:51] start calculating std\n",
      "[15:41:51] chunk 93/234 (920000:930000)--------\n",
      "[15:41:51] start predicting and padding\n",
      "[15:42:47] start calculating std\n",
      "[15:42:47] chunk 94/234 (930000:940000)--------\n",
      "[15:42:47] start predicting and padding\n",
      "[15:43:42] start calculating std\n",
      "[15:43:43] chunk 95/234 (940000:950000)--------\n",
      "[15:43:43] start predicting and padding\n",
      "[15:44:38] start calculating std\n",
      "[15:44:39] chunk 96/234 (950000:960000)--------\n",
      "[15:44:39] start predicting and padding\n",
      "[15:45:35] start calculating std\n",
      "[15:45:35] chunk 97/234 (960000:970000)--------\n",
      "[15:45:35] start predicting and padding\n",
      "[15:46:30] start calculating std\n",
      "[15:46:31] chunk 98/234 (970000:980000)--------\n",
      "[15:46:31] start predicting and padding\n",
      "[15:47:24] start calculating std\n",
      "[15:47:25] chunk 99/234 (980000:990000)--------\n",
      "[15:47:25] start predicting and padding\n",
      "[15:48:18] start calculating std\n",
      "[15:48:18] chunk 100/234 (990000:1000000)--------\n",
      "[15:48:18] start predicting and padding\n",
      "[15:49:11] start calculating std\n",
      "[15:49:12] chunk 101/234 (1000000:1010000)--------\n",
      "[15:49:12] start predicting and padding\n",
      "[15:50:05] start calculating std\n",
      "[15:50:06] chunk 102/234 (1010000:1020000)--------\n",
      "[15:50:06] start predicting and padding\n",
      "[15:50:59] start calculating std\n",
      "[15:50:59] chunk 103/234 (1020000:1030000)--------\n",
      "[15:50:59] start predicting and padding\n",
      "[15:51:53] start calculating std\n",
      "[15:51:53] chunk 104/234 (1030000:1040000)--------\n",
      "[15:51:53] start predicting and padding\n",
      "[15:52:47] start calculating std\n",
      "[15:52:47] chunk 105/234 (1040000:1050000)--------\n",
      "[15:52:47] start predicting and padding\n",
      "[15:53:40] start calculating std\n",
      "[15:53:40] chunk 106/234 (1050000:1060000)--------\n",
      "[15:53:40] start predicting and padding\n",
      "[15:54:33] start calculating std\n",
      "[15:54:34] chunk 107/234 (1060000:1070000)--------\n",
      "[15:54:34] start predicting and padding\n",
      "[15:55:26] start calculating std\n",
      "[15:55:26] chunk 108/234 (1070000:1080000)--------\n",
      "[15:55:26] start predicting and padding\n",
      "[15:56:19] start calculating std\n",
      "[15:56:19] chunk 109/234 (1080000:1090000)--------\n",
      "[15:56:19] start predicting and padding\n",
      "[15:57:12] start calculating std\n",
      "[15:57:12] chunk 110/234 (1090000:1100000)--------\n",
      "[15:57:12] start predicting and padding\n",
      "[15:58:05] start calculating std\n",
      "[15:58:05] chunk 111/234 (1100000:1110000)--------\n",
      "[15:58:05] start predicting and padding\n",
      "[15:58:58] start calculating std\n",
      "[15:58:59] chunk 112/234 (1110000:1120000)--------\n",
      "[15:58:59] start predicting and padding\n",
      "[15:59:51] start calculating std\n",
      "[15:59:52] chunk 113/234 (1120000:1130000)--------\n",
      "[15:59:52] start predicting and padding\n",
      "[16:00:44] start calculating std\n",
      "[16:00:45] chunk 114/234 (1130000:1140000)--------\n",
      "[16:00:45] start predicting and padding\n",
      "[16:01:37] start calculating std\n",
      "[16:01:38] chunk 115/234 (1140000:1150000)--------\n",
      "[16:01:38] start predicting and padding\n",
      "[16:02:30] start calculating std\n",
      "[16:02:31] chunk 116/234 (1150000:1160000)--------\n",
      "[16:02:31] start predicting and padding\n",
      "[16:03:23] start calculating std\n",
      "[16:03:23] chunk 117/234 (1160000:1170000)--------\n",
      "[16:03:23] start predicting and padding\n",
      "[16:04:16] start calculating std\n",
      "[16:04:16] chunk 118/234 (1170000:1180000)--------\n",
      "[16:04:16] start predicting and padding\n",
      "[16:05:08] start calculating std\n",
      "[16:05:08] chunk 119/234 (1180000:1190000)--------\n",
      "[16:05:08] start predicting and padding\n",
      "[16:06:01] start calculating std\n",
      "[16:06:01] chunk 120/234 (1190000:1200000)--------\n",
      "[16:06:01] start predicting and padding\n",
      "[16:06:54] start calculating std\n",
      "[16:06:54] chunk 121/234 (1200000:1210000)--------\n",
      "[16:06:54] start predicting and padding\n",
      "[16:07:47] start calculating std\n",
      "[16:07:47] chunk 122/234 (1210000:1220000)--------\n",
      "[16:07:47] start predicting and padding\n",
      "[16:08:40] start calculating std\n",
      "[16:08:40] chunk 123/234 (1220000:1230000)--------\n",
      "[16:08:40] start predicting and padding\n",
      "[16:09:33] start calculating std\n",
      "[16:09:33] chunk 124/234 (1230000:1240000)--------\n",
      "[16:09:33] start predicting and padding\n",
      "[16:10:26] start calculating std\n",
      "[16:10:26] chunk 125/234 (1240000:1250000)--------\n",
      "[16:10:26] start predicting and padding\n",
      "[16:11:19] start calculating std\n",
      "[16:11:19] chunk 126/234 (1250000:1260000)--------\n",
      "[16:11:19] start predicting and padding\n",
      "[16:12:12] start calculating std\n",
      "[16:12:13] chunk 127/234 (1260000:1270000)--------\n",
      "[16:12:13] start predicting and padding\n",
      "[16:13:06] start calculating std\n",
      "[16:13:06] chunk 128/234 (1270000:1280000)--------\n",
      "[16:13:06] start predicting and padding\n",
      "[16:13:59] start calculating std\n",
      "[16:13:59] chunk 129/234 (1280000:1290000)--------\n",
      "[16:13:59] start predicting and padding\n",
      "[16:14:52] start calculating std\n",
      "[16:14:53] chunk 130/234 (1290000:1300000)--------\n",
      "[16:14:53] start predicting and padding\n",
      "[16:15:45] start calculating std\n",
      "[16:15:45] chunk 131/234 (1300000:1310000)--------\n",
      "[16:15:45] start predicting and padding\n",
      "[16:16:39] start calculating std\n",
      "[16:16:39] chunk 132/234 (1310000:1320000)--------\n",
      "[16:16:39] start predicting and padding\n",
      "[16:17:32] start calculating std\n",
      "[16:17:32] chunk 133/234 (1320000:1330000)--------\n",
      "[16:17:32] start predicting and padding\n",
      "[16:18:25] start calculating std\n",
      "[16:18:26] chunk 134/234 (1330000:1340000)--------\n",
      "[16:18:26] start predicting and padding\n",
      "[16:19:19] start calculating std\n",
      "[16:19:19] chunk 135/234 (1340000:1350000)--------\n",
      "[16:19:19] start predicting and padding\n",
      "[16:20:12] start calculating std\n",
      "[16:20:12] chunk 136/234 (1350000:1360000)--------\n",
      "[16:20:12] start predicting and padding\n",
      "[16:21:06] start calculating std\n",
      "[16:21:06] chunk 137/234 (1360000:1370000)--------\n",
      "[16:21:06] start predicting and padding\n",
      "[16:22:00] start calculating std\n",
      "[16:22:00] chunk 138/234 (1370000:1380000)--------\n",
      "[16:22:00] start predicting and padding\n",
      "[16:22:55] start calculating std\n",
      "[16:22:55] chunk 139/234 (1380000:1390000)--------\n",
      "[16:22:55] start predicting and padding\n",
      "[16:23:49] start calculating std\n",
      "[16:23:50] chunk 140/234 (1390000:1400000)--------\n",
      "[16:23:50] start predicting and padding\n",
      "[16:24:44] start calculating std\n",
      "[16:24:45] chunk 141/234 (1400000:1410000)--------\n",
      "[16:24:45] start predicting and padding\n",
      "[16:25:38] start calculating std\n",
      "[16:25:39] chunk 142/234 (1410000:1420000)--------\n",
      "[16:25:39] start predicting and padding\n",
      "[16:26:32] start calculating std\n",
      "[16:26:33] chunk 143/234 (1420000:1430000)--------\n",
      "[16:26:33] start predicting and padding\n",
      "[16:27:27] start calculating std\n",
      "[16:27:28] chunk 144/234 (1430000:1440000)--------\n",
      "[16:27:28] start predicting and padding\n",
      "[16:28:22] start calculating std\n",
      "[16:28:23] chunk 145/234 (1440000:1450000)--------\n",
      "[16:28:23] start predicting and padding\n",
      "[16:29:17] start calculating std\n",
      "[16:29:18] chunk 146/234 (1450000:1460000)--------\n",
      "[16:29:18] start predicting and padding\n",
      "[16:30:13] start calculating std\n",
      "[16:30:14] chunk 147/234 (1460000:1470000)--------\n",
      "[16:30:14] start predicting and padding\n",
      "[16:31:08] start calculating std\n",
      "[16:31:09] chunk 148/234 (1470000:1480000)--------\n",
      "[16:31:09] start predicting and padding\n",
      "[16:32:04] start calculating std\n",
      "[16:32:05] chunk 149/234 (1480000:1490000)--------\n",
      "[16:32:05] start predicting and padding\n",
      "[16:33:01] start calculating std\n",
      "[16:33:02] chunk 150/234 (1490000:1500000)--------\n",
      "[16:33:02] start predicting and padding\n",
      "[16:33:57] start calculating std\n",
      "[16:33:58] chunk 151/234 (1500000:1510000)--------\n",
      "[16:33:58] start predicting and padding\n",
      "[16:34:53] start calculating std\n",
      "[16:34:54] chunk 152/234 (1510000:1520000)--------\n",
      "[16:34:54] start predicting and padding\n",
      "[16:35:48] start calculating std\n",
      "[16:35:49] chunk 153/234 (1520000:1530000)--------\n",
      "[16:35:49] start predicting and padding\n",
      "[16:36:44] start calculating std\n",
      "[16:36:45] chunk 154/234 (1530000:1540000)--------\n",
      "[16:36:45] start predicting and padding\n",
      "[16:37:40] start calculating std\n",
      "[16:37:40] chunk 155/234 (1540000:1550000)--------\n",
      "[16:37:40] start predicting and padding\n",
      "[16:38:35] start calculating std\n",
      "[16:38:36] chunk 156/234 (1550000:1560000)--------\n",
      "[16:38:36] start predicting and padding\n",
      "[16:39:31] start calculating std\n",
      "[16:39:32] chunk 157/234 (1560000:1570000)--------\n",
      "[16:39:32] start predicting and padding\n",
      "[16:40:27] start calculating std\n",
      "[16:40:28] chunk 158/234 (1570000:1580000)--------\n",
      "[16:40:28] start predicting and padding\n",
      "[16:41:22] start calculating std\n",
      "[16:41:23] chunk 159/234 (1580000:1590000)--------\n",
      "[16:41:23] start predicting and padding\n",
      "[16:42:17] start calculating std\n",
      "[16:42:18] chunk 160/234 (1590000:1600000)--------\n",
      "[16:42:18] start predicting and padding\n",
      "[16:43:11] start calculating std\n",
      "[16:43:12] chunk 161/234 (1600000:1610000)--------\n",
      "[16:43:12] start predicting and padding\n",
      "[16:44:05] start calculating std\n",
      "[16:44:06] chunk 162/234 (1610000:1620000)--------\n",
      "[16:44:06] start predicting and padding\n",
      "[16:44:59] start calculating std\n",
      "[16:45:00] chunk 163/234 (1620000:1630000)--------\n",
      "[16:45:00] start predicting and padding\n",
      "[16:45:52] start calculating std\n",
      "[16:45:53] chunk 164/234 (1630000:1640000)--------\n",
      "[16:45:53] start predicting and padding\n",
      "[16:46:46] start calculating std\n",
      "[16:46:46] chunk 165/234 (1640000:1650000)--------\n",
      "[16:46:46] start predicting and padding\n",
      "[16:47:38] start calculating std\n",
      "[16:47:38] chunk 166/234 (1650000:1660000)--------\n",
      "[16:47:38] start predicting and padding\n",
      "[16:48:32] start calculating std\n",
      "[16:48:33] chunk 167/234 (1660000:1670000)--------\n",
      "[16:48:33] start predicting and padding\n",
      "[16:49:26] start calculating std\n",
      "[16:49:27] chunk 168/234 (1670000:1680000)--------\n",
      "[16:49:27] start predicting and padding\n",
      "[16:50:20] start calculating std\n",
      "[16:50:21] chunk 169/234 (1680000:1690000)--------\n",
      "[16:50:21] start predicting and padding\n",
      "[16:51:14] start calculating std\n",
      "[16:51:15] chunk 170/234 (1690000:1700000)--------\n",
      "[16:51:15] start predicting and padding\n",
      "[16:52:08] start calculating std\n",
      "[16:52:09] chunk 171/234 (1700000:1710000)--------\n",
      "[16:52:09] start predicting and padding\n",
      "[16:53:02] start calculating std\n",
      "[16:53:03] chunk 172/234 (1710000:1720000)--------\n",
      "[16:53:03] start predicting and padding\n",
      "[16:53:57] start calculating std\n",
      "[16:53:57] chunk 173/234 (1720000:1730000)--------\n",
      "[16:53:57] start predicting and padding\n",
      "[16:54:51] start calculating std\n",
      "[16:54:52] chunk 174/234 (1730000:1740000)--------\n",
      "[16:54:52] start predicting and padding\n",
      "[16:55:46] start calculating std\n",
      "[16:55:47] chunk 175/234 (1740000:1750000)--------\n",
      "[16:55:47] start predicting and padding\n",
      "[16:56:41] start calculating std\n",
      "[16:56:41] chunk 176/234 (1750000:1760000)--------\n",
      "[16:56:41] start predicting and padding\n",
      "[16:57:35] start calculating std\n",
      "[16:57:36] chunk 177/234 (1760000:1770000)--------\n",
      "[16:57:36] start predicting and padding\n",
      "[16:58:29] start calculating std\n",
      "[16:58:30] chunk 178/234 (1770000:1780000)--------\n",
      "[16:58:30] start predicting and padding\n",
      "[16:59:24] start calculating std\n",
      "[16:59:25] chunk 179/234 (1780000:1790000)--------\n",
      "[16:59:25] start predicting and padding\n",
      "[17:00:19] start calculating std\n",
      "[17:00:19] chunk 180/234 (1790000:1800000)--------\n",
      "[17:00:19] start predicting and padding\n",
      "[17:01:13] start calculating std\n",
      "[17:01:14] chunk 181/234 (1800000:1810000)--------\n",
      "[17:01:14] start predicting and padding\n",
      "[17:02:09] start calculating std\n",
      "[17:02:09] chunk 182/234 (1810000:1820000)--------\n",
      "[17:02:09] start predicting and padding\n",
      "[17:03:04] start calculating std\n",
      "[17:03:05] chunk 183/234 (1820000:1830000)--------\n",
      "[17:03:05] start predicting and padding\n",
      "[17:03:59] start calculating std\n",
      "[17:03:59] chunk 184/234 (1830000:1840000)--------\n",
      "[17:03:59] start predicting and padding\n",
      "[17:04:53] start calculating std\n",
      "[17:04:54] chunk 185/234 (1840000:1850000)--------\n",
      "[17:04:54] start predicting and padding\n",
      "[17:05:48] start calculating std\n",
      "[17:05:48] chunk 186/234 (1850000:1860000)--------\n",
      "[17:05:48] start predicting and padding\n",
      "[17:06:42] start calculating std\n",
      "[17:06:43] chunk 187/234 (1860000:1870000)--------\n",
      "[17:06:43] start predicting and padding\n",
      "[17:07:36] start calculating std\n",
      "[17:07:37] chunk 188/234 (1870000:1880000)--------\n",
      "[17:07:37] start predicting and padding\n",
      "[17:08:30] start calculating std\n",
      "[17:08:31] chunk 189/234 (1880000:1890000)--------\n",
      "[17:08:31] start predicting and padding\n",
      "[17:09:24] start calculating std\n",
      "[17:09:24] chunk 190/234 (1890000:1900000)--------\n",
      "[17:09:24] start predicting and padding\n",
      "[17:10:18] start calculating std\n",
      "[17:10:18] chunk 191/234 (1900000:1910000)--------\n",
      "[17:10:18] start predicting and padding\n",
      "[17:11:12] start calculating std\n",
      "[17:11:12] chunk 192/234 (1910000:1920000)--------\n",
      "[17:11:12] start predicting and padding\n",
      "[17:12:06] start calculating std\n",
      "[17:12:06] chunk 193/234 (1920000:1930000)--------\n",
      "[17:12:06] start predicting and padding\n",
      "[17:13:00] start calculating std\n",
      "[17:13:00] chunk 194/234 (1930000:1940000)--------\n",
      "[17:13:00] start predicting and padding\n",
      "[17:13:54] start calculating std\n",
      "[17:13:54] chunk 195/234 (1940000:1950000)--------\n",
      "[17:13:54] start predicting and padding\n",
      "[17:14:47] start calculating std\n",
      "[17:14:48] chunk 196/234 (1950000:1960000)--------\n",
      "[17:14:48] start predicting and padding\n",
      "[17:15:41] start calculating std\n",
      "[17:15:42] chunk 197/234 (1960000:1970000)--------\n",
      "[17:15:42] start predicting and padding\n",
      "[17:16:35] start calculating std\n",
      "[17:16:35] chunk 198/234 (1970000:1980000)--------\n",
      "[17:16:35] start predicting and padding\n",
      "[17:17:29] start calculating std\n",
      "[17:17:29] chunk 199/234 (1980000:1990000)--------\n",
      "[17:17:29] start predicting and padding\n",
      "[17:18:22] start calculating std\n",
      "[17:18:23] chunk 200/234 (1990000:2000000)--------\n",
      "[17:18:23] start predicting and padding\n",
      "[17:19:16] start calculating std\n",
      "[17:19:16] chunk 201/234 (2000000:2010000)--------\n",
      "[17:19:16] start predicting and padding\n",
      "[17:20:09] start calculating std\n",
      "[17:20:10] chunk 202/234 (2010000:2020000)--------\n",
      "[17:20:10] start predicting and padding\n",
      "[17:21:02] start calculating std\n",
      "[17:21:03] chunk 203/234 (2020000:2030000)--------\n",
      "[17:21:03] start predicting and padding\n",
      "[17:21:56] start calculating std\n",
      "[17:21:56] chunk 204/234 (2030000:2040000)--------\n",
      "[17:21:56] start predicting and padding\n",
      "[17:22:49] start calculating std\n",
      "[17:22:50] chunk 205/234 (2040000:2050000)--------\n",
      "[17:22:50] start predicting and padding\n",
      "[17:23:43] start calculating std\n",
      "[17:23:43] chunk 206/234 (2050000:2060000)--------\n",
      "[17:23:43] start predicting and padding\n",
      "[17:24:37] start calculating std\n",
      "[17:24:37] chunk 207/234 (2060000:2070000)--------\n",
      "[17:24:37] start predicting and padding\n",
      "[17:25:31] start calculating std\n",
      "[17:25:31] chunk 208/234 (2070000:2080000)--------\n",
      "[17:25:31] start predicting and padding\n",
      "[17:26:25] start calculating std\n",
      "[17:26:25] chunk 209/234 (2080000:2090000)--------\n",
      "[17:26:25] start predicting and padding\n",
      "[17:27:18] start calculating std\n",
      "[17:27:18] chunk 210/234 (2090000:2100000)--------\n",
      "[17:27:18] start predicting and padding\n",
      "[17:28:12] start calculating std\n",
      "[17:28:12] chunk 211/234 (2100000:2110000)--------\n",
      "[17:28:12] start predicting and padding\n",
      "[17:29:05] start calculating std\n",
      "[17:29:05] chunk 212/234 (2110000:2120000)--------\n",
      "[17:29:05] start predicting and padding\n",
      "[17:29:58] start calculating std\n",
      "[17:29:59] chunk 213/234 (2120000:2130000)--------\n",
      "[17:29:59] start predicting and padding\n",
      "[17:30:52] start calculating std\n",
      "[17:30:52] chunk 214/234 (2130000:2140000)--------\n",
      "[17:30:52] start predicting and padding\n",
      "[17:31:45] start calculating std\n",
      "[17:31:45] chunk 215/234 (2140000:2150000)--------\n",
      "[17:31:45] start predicting and padding\n",
      "[17:32:39] start calculating std\n",
      "[17:32:39] chunk 216/234 (2150000:2160000)--------\n",
      "[17:32:39] start predicting and padding\n",
      "[17:33:32] start calculating std\n",
      "[17:33:32] chunk 217/234 (2160000:2170000)--------\n",
      "[17:33:32] start predicting and padding\n",
      "[17:34:25] start calculating std\n",
      "[17:34:25] chunk 218/234 (2170000:2180000)--------\n",
      "[17:34:25] start predicting and padding\n",
      "[17:35:18] start calculating std\n",
      "[17:35:18] chunk 219/234 (2180000:2190000)--------\n",
      "[17:35:18] start predicting and padding\n",
      "[17:36:11] start calculating std\n",
      "[17:36:11] chunk 220/234 (2190000:2200000)--------\n",
      "[17:36:11] start predicting and padding\n",
      "[17:37:04] start calculating std\n",
      "[17:37:05] chunk 221/234 (2200000:2210000)--------\n",
      "[17:37:05] start predicting and padding\n",
      "[17:37:58] start calculating std\n",
      "[17:37:58] chunk 222/234 (2210000:2220000)--------\n",
      "[17:37:58] start predicting and padding\n",
      "[17:38:50] start calculating std\n",
      "[17:38:50] chunk 223/234 (2220000:2230000)--------\n",
      "[17:38:50] start predicting and padding\n",
      "[17:39:44] start calculating std\n",
      "[17:39:44] chunk 224/234 (2230000:2240000)--------\n",
      "[17:39:44] start predicting and padding\n",
      "[17:40:36] start calculating std\n",
      "[17:40:36] chunk 225/234 (2240000:2250000)--------\n",
      "[17:40:36] start predicting and padding\n",
      "[17:41:29] start calculating std\n",
      "[17:41:29] chunk 226/234 (2250000:2260000)--------\n",
      "[17:41:29] start predicting and padding\n",
      "[17:42:21] start calculating std\n",
      "[17:42:21] chunk 227/234 (2260000:2270000)--------\n",
      "[17:42:21] start predicting and padding\n",
      "[17:43:14] start calculating std\n",
      "[17:43:14] chunk 228/234 (2270000:2280000)--------\n",
      "[17:43:14] start predicting and padding\n",
      "[17:44:07] start calculating std\n",
      "[17:44:07] chunk 229/234 (2280000:2290000)--------\n",
      "[17:44:07] start predicting and padding\n",
      "[17:45:00] start calculating std\n",
      "[17:45:01] chunk 230/234 (2290000:2300000)--------\n",
      "[17:45:01] start predicting and padding\n",
      "[17:45:54] start calculating std\n",
      "[17:45:54] chunk 231/234 (2300000:2310000)--------\n",
      "[17:45:54] start predicting and padding\n",
      "[17:46:46] start calculating std\n",
      "[17:46:47] chunk 232/234 (2310000:2320000)--------\n",
      "[17:46:47] start predicting and padding\n",
      "[17:47:39] start calculating std\n",
      "[17:47:39] chunk 233/234 (2320000:2330000)--------\n",
      "[17:47:39] start predicting and padding\n",
      "[17:48:32] start calculating std\n",
      "[17:48:32] chunk 234/234 (2330000:2335589)--------\n",
      "[17:48:32] start predicting and padding\n",
      "[17:49:02] start calculating std\n",
      "[17:49:02] finish all-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65218/720670535.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dff['pred_std'] = np.concatenate(pred_std_chunks)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'material'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m ttprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinish all-------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m dff[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_std\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(pred_std_chunks)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mdff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./material/pixel_agg.predicted_all.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mversion\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.gpkg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:2889\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2885\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2886\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2890\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2897\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2898\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parquet.py:411\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    409\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 411\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parquet.py:161\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     from_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreserve_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[1;32m    159\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(df, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_pandas_kwargs)\n\u001b[0;32m--> 161\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilesystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(path_or_handle, io\u001b[38;5;241m.\u001b[39mBufferedWriter)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(path_or_handle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_handle\u001b[38;5;241m.\u001b[39mname, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m))\n\u001b[1;32m    172\u001b[0m ):\n\u001b[1;32m    173\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m path_or_handle\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parquet.py:110\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    100\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:737\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 737\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:600\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    598\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'material'"
     ]
    }
   ],
   "source": [
    "# do the predictions in chunck\n",
    "chunk_size = 10000\n",
    "num_chunks = int(np.ceil(len(dff) / chunk_size))\n",
    "\n",
    "pred_std_chunks = []\n",
    "\n",
    "ttprint(f'In total {num_chunks} chunks')\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = min((i + 1) * chunk_size, len(dff))\n",
    "    ttprint(f'chunk {i + 1}/{num_chunks} ({start}:{end})--------')\n",
    "\n",
    "    chunk = dff.iloc[start:end]\n",
    "\n",
    "    ttprint('start predicting and padding')\n",
    "    node_preds_chunk = modeln.predict(chunk[covs])\n",
    "    nodes_chunk = pad_leaf_outputs_to_array(node_preds_chunk, pad_value=np.nan)\n",
    "    nodes_chunk = np.expm1(nodes_chunk)\n",
    "    \n",
    "    ttprint('start calculating std')\n",
    "    std_chunk = np.nanstd(nodes_chunk.T, axis=0)\n",
    "    pred_std_chunks.append(std_chunk)\n",
    "\n",
    "ttprint(f'finish all-------------------------------')\n",
    "\n",
    "dff['pred_std'] = np.concatenate(pred_std_chunks)\n",
    "\n",
    "dff.to_parquet(f'pixel_agg.predicted_all.{version}.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d26ed99-bb39-4669-9e12-d11d97cf4b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nuts0</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>pred_2009</th>\n",
       "      <th>pred_2018</th>\n",
       "      <th>pred_std_2009</th>\n",
       "      <th>pred_std_2018</th>\n",
       "      <th>ndvi_2009</th>\n",
       "      <th>ndvi_2018</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE</td>\n",
       "      <td>4031500.0</td>\n",
       "      <td>2684510.0</td>\n",
       "      <td>4031500.0</td>\n",
       "      <td>2684510.0</td>\n",
       "      <td>50.580191</td>\n",
       "      <td>51.958039</td>\n",
       "      <td>55.865364</td>\n",
       "      <td>58.576836</td>\n",
       "      <td>221.0</td>\n",
       "      <td>221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE</td>\n",
       "      <td>4031500.0</td>\n",
       "      <td>2685510.0</td>\n",
       "      <td>4031500.0</td>\n",
       "      <td>2685510.0</td>\n",
       "      <td>61.634346</td>\n",
       "      <td>50.056710</td>\n",
       "      <td>63.864868</td>\n",
       "      <td>60.460979</td>\n",
       "      <td>223.0</td>\n",
       "      <td>226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE</td>\n",
       "      <td>4031500.0</td>\n",
       "      <td>2686510.0</td>\n",
       "      <td>4031500.0</td>\n",
       "      <td>2686510.0</td>\n",
       "      <td>46.685048</td>\n",
       "      <td>56.671120</td>\n",
       "      <td>37.056778</td>\n",
       "      <td>56.921967</td>\n",
       "      <td>225.0</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE</td>\n",
       "      <td>4031500.0</td>\n",
       "      <td>2687510.0</td>\n",
       "      <td>4031500.0</td>\n",
       "      <td>2687510.0</td>\n",
       "      <td>45.154006</td>\n",
       "      <td>43.176903</td>\n",
       "      <td>39.046261</td>\n",
       "      <td>38.734066</td>\n",
       "      <td>220.0</td>\n",
       "      <td>223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>4031500.0</td>\n",
       "      <td>2688510.0</td>\n",
       "      <td>4031500.0</td>\n",
       "      <td>2688510.0</td>\n",
       "      <td>26.473726</td>\n",
       "      <td>24.404207</td>\n",
       "      <td>36.337608</td>\n",
       "      <td>26.744663</td>\n",
       "      <td>199.0</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  nuts0        lon        lat          x          y  pred_2009  pred_2018  \\\n",
       "0    DE  4031500.0  2684510.0  4031500.0  2684510.0  50.580191  51.958039   \n",
       "1    DE  4031500.0  2685510.0  4031500.0  2685510.0  61.634346  50.056710   \n",
       "2    DE  4031500.0  2686510.0  4031500.0  2686510.0  46.685048  56.671120   \n",
       "3    DE  4031500.0  2687510.0  4031500.0  2687510.0  45.154006  43.176903   \n",
       "4    DE  4031500.0  2688510.0  4031500.0  2688510.0  26.473726  24.404207   \n",
       "\n",
       "   pred_std_2009  pred_std_2018  ndvi_2009  ndvi_2018  \n",
       "0      55.865364      58.576836      221.0      221.0  \n",
       "1      63.864868      60.460979      223.0      226.0  \n",
       "2      37.056778      56.921967      225.0      231.0  \n",
       "3      39.046261      38.734066      220.0      223.0  \n",
       "4      36.337608      26.744663      199.0      194.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = dff.rename(columns={'ndvi_glad.landast.ard2.seasconv.m.yearly_p50_30m_s_YYYY0101_YYYY1231_eu_epsg.3035_v20231127':'ndvi'})\n",
    "\n",
    "out = (\n",
    "    dff\n",
    "    .pivot_table(\n",
    "        index=['nuts0', 'lon', 'lat', 'x', 'y'],\n",
    "        columns='time',\n",
    "        values=['pred', 'pred_std', 'ndvi'],\n",
    "        aggfunc='first'       \n",
    "    )\n",
    "    .dropna(axis=0)           \n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "out.columns = [\n",
    "    f'{c[0]}_{c[1]}' if c[1] else c[0]   \n",
    "    for c in out.columns.to_flat_index()\n",
    "]\n",
    "\n",
    "static_cols = ['nuts0', 'lon', 'lat', 'x', 'y']\n",
    "value_cols  = ['pred_2009', 'pred_2018', 'pred_std_2009', 'pred_std_2018', 'ndvi_2009', 'ndvi_2018']\n",
    "out = out[static_cols + value_cols]\n",
    "\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5846be0-6b72-4e1c-8c11-09d533f93ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "out['change'] = out['pred_2018'] - out['pred_2009']\n",
    "out['noise'] = np.sqrt(out['pred_std_2009']**2 + out['pred_std_2018']**2)\n",
    "out['signal'] = out['change'].abs()\n",
    "out['SNR'] = out['signal']/out['noise']\n",
    "out['ndvi_mean'] = (out['ndvi_2009'] + out['ndvi_2018'])/2\n",
    "out['ndvi_mean'] = (out['ndvi_mean'] - 125)/125\n",
    "out = out.drop(columns=['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9248fde6-d3c2-49c3-9662-d7d1c8b0c8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out.to_parquet('agg_pixel_change.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfda959-af14-4846-b040-3cc7145e280f",
   "metadata": {},
   "source": [
    "## save as geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d02da91-c87b-47c4-9cd6-d823aecbdc6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geometry = [Point(xy) for xy in zip(out['lon'], out['lat'])]\n",
    "gdf = gpd.GeoDataFrame(out, geometry=geometry, crs=\"EPSG:3035\")\n",
    "de = gpd.read_file(\"nuts_de_2021.gpkg\")         \n",
    "es = gpd.read_file(\"nuts_es_2021.gpkg\")         \n",
    "poly = gpd.GeoDataFrame(pd.concat([es, de], ignore_index=True), crs=\"EPSG:3035\")\n",
    "gdf = gpd.sjoin(gdf, poly, how=\"inner\", predicate=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25350758-7a75-4015-a496-4ef0a6d232f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoTIFF written to predictions.tif\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "value_col = \"SNR\"        \n",
    "points_gdf = gdf.dropna(subset=[value_col]) \n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  DERIVE GRID RESOLUTION & EXTENT ------------------------------\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "x_res = 1000    # ≈ pixel width  (e.g. 1000 m)\n",
    "y_res = 1000           # ≈ pixel height (e.g. 1000 m)\n",
    "\n",
    "minx, miny, maxx, maxy = points_gdf.total_bounds\n",
    "\n",
    "# number of pixels in each direction (+1 so last column/row is included)\n",
    "width  = int(round((maxx - minx) / x_res)) + 1\n",
    "height = int(round((maxy - miny) / y_res)) + 1\n",
    "\n",
    "# Affine transform (upper-left corner is half a pixel north-west of first point)\n",
    "transform = from_origin(minx - x_res / 2,   # west-most edge\n",
    "                        maxy + y_res / 2,   # north-most edge\n",
    "                        x_res,\n",
    "                        y_res)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  RASTERIZE POINT VALUES ---------------------------------------\n",
    "# ------------------------------------------------------------------\n",
    "shapes = ((geom, val) for geom, val in\n",
    "          zip(points_gdf.geometry, points_gdf[value_col]))\n",
    "\n",
    "raster = rasterize(\n",
    "    shapes=shapes,\n",
    "    out_shape=(height, width),\n",
    "    transform=transform,\n",
    "    fill=np.nan,             # background\n",
    "    dtype=\"float32\"          # change if your data need another type\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  WRITE GeoTIFF -----------------------------------------------\n",
    "# ------------------------------------------------------------------\n",
    "meta = {\n",
    "    \"driver\":  \"GTiff\",\n",
    "    \"height\":  height,\n",
    "    \"width\":   width,\n",
    "    \"count\":   1,\n",
    "    \"dtype\":   \"float32\",\n",
    "    \"crs\":     points_gdf.crs,   # keep original CRS\n",
    "    \"transform\": transform,\n",
    "    \"nodata\":  np.nan\n",
    "}\n",
    "\n",
    "with rasterio.open(f\"{value_col}.tif\", \"w\", **meta) as dst:\n",
    "    dst.write(raster, 1)\n",
    "\n",
    "print(\"GeoTIFF written to predictions.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ae826-252a-4dc0-b4f3-5884acac667e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
